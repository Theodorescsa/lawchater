import os
import re
import unicodedata
from pathlib import Path

# AI Libraries
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import LlamaCpp
from langchain_core.prompts import PromptTemplate
from sentence_transformers import CrossEncoder

os.environ["ANONYMIZED_TELEMETRY"] = "False"

class RAGService:
    _instance = None
    _initialized = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RAGService, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._initialize()
            self._initialized = True

    def _initialize(self):
        print("üî• ƒêang kh·ªüi t·∫°o LawChatter RAG Engine (LlamaCpp + Rerank)...")
        
        # --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
        # L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c 'core' hi·ªán t·∫°i
        self.BASE_DIR = Path(__file__).resolve().parent
        
        # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n MODEL t·∫°i core/models/
        self.MODEL_FILENAME = "qwen2.5-3b-instruct-q5_k_m.gguf" # <-- T√™n file model c·ªßa b·∫°n
        self.MODEL_PATH = str(self.BASE_DIR / "models" / self.MODEL_FILENAME)
        
        self.PERSIST_PATH = str(self.BASE_DIR / "chroma_db")
        self.EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-small"
        self.RERANK_MODEL_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"
        self.COLLECTION_NAME = "law_docs"

        # Ki·ªÉm tra file model t·ªìn t·∫°i ch∆∞a
        if not os.path.exists(self.MODEL_PATH):
            raise FileNotFoundError(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y model t·∫°i {self.MODEL_PATH}. Vui l√≤ng ki·ªÉm tra folder core/models/")

        # 1. Kh·ªüi t·∫°o LLM (LlamaCpp)
        print(f"‚è≥ Loading LLM from {self.MODEL_PATH}...")
        self.llm = LlamaCpp(
            model_path=self.MODEL_PATH,
            n_gpu_layers=-1,      # ƒê·∫©y 100% layers l√™n GPU
            n_batch=512,
            n_ctx=4096,
            max_tokens=2048,
            temperature=0.1,
            top_p=0.9,
            repeat_penalty=1.15,
            verbose=False,        # T·∫Øt log r√°c trong console Django
            stop=["<|im_end|>", "Ng∆∞·ªùi d√πng:", "K·∫øt th√∫c"]
        )

        # 2. Embedding & Vector Store
        print("‚è≥ Loading Embedding Model...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=self.EMBEDDING_MODEL_NAME, 
            model_kwargs={"device": "cpu"} # ChromaDB ch·∫°y CPU ƒë·ªÉ ti·∫øt ki·ªám VRAM cho LLM
        )
        
        self.vectorstore = Chroma(
            collection_name=self.COLLECTION_NAME,
            persist_directory=self.PERSIST_PATH,
            embedding_function=self.embedding_model
        )

        # 3. Reranker (Cross-Encoder)
        print("‚è≥ Loading Reranker Model...")
        self.reranker = CrossEncoder(self.RERANK_MODEL_NAME)

        # 4. Prompt Template (ƒê√£ t·ªëi ∆∞u cho Qwen/Llama)
        template = """<|im_start|>system
B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω ·∫£o LawChatter.

NHI·ªÜM V·ª§:
S·ª≠ d·ª•ng th√¥ng tin trong th·∫ª <documents> ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.

QUY TR√åNH SUY LU·∫¨N:
1. X√°c ƒë·ªãnh ƒë√∫ng ƒëo·∫°n vƒÉn b·∫£n ch·ª©a c√¢u tr·∫£ l·ªùi.
2. Tr√≠ch xu·∫•t "S·ªë hi·ªáu ƒëi·ªÅu lu·∫≠t" (VD: ƒêi·ªÅu 168, ƒêi·ªÅu 20...).
3. T·ªïng h·ª£p ƒë·∫ßy ƒë·ªß quy ƒë·ªãnh/khung h√¨nh ph·∫°t.

Y√äU C·∫¶U ƒê·∫¶U RA:
- B·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi b·∫±ng: "Theo quy ƒë·ªãnh t·∫°i [S·ªë hi·ªáu ƒëi·ªÅu lu·∫≠t]..."
- Tr√¨nh b√†y chi ti·∫øt, g·∫°ch ƒë·∫ßu d√≤ng r√µ r√†ng.
- K·∫øt th√∫c b·∫±ng: (Ngu·ªìn: [T√™n_File])

V√ç D·ª§ M·∫™U:
C√¢u h·ªèi: T·ªôi c∆∞·ªõp t√†i s·∫£n b·ªã ph·∫°t th·∫ø n√†o?
Tr·∫£ l·ªùi:
Theo quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 168 B·ªô lu·∫≠t H√¨nh s·ª±:
- Ng∆∞·ªùi n√†o d√πng v≈© l·ª±c ƒëe d·ªça chi·∫øm ƒëo·∫°t t√†i s·∫£n th√¨ b·ªã ph·∫°t t√π t·ª´ 03 nƒÉm ƒë·∫øn 10 nƒÉm.
(Ngu·ªìn: BLHS.docx)
<|im_end|>
<|im_start|>user
D·ªÆ LI·ªÜU LU·∫¨T (XML):
{context}

C√¢u h·ªèi: {question}
<|im_end|>
<|im_start|>assistant
C√¢u tr·∫£ l·ªùi:"""
        self.prompt_template = PromptTemplate(input_variables=["context", "question"], template=template)

        print("‚úÖ RAG Service Ready!")

    # --- C√ÅC H√ÄM H·ªñ TR·ª¢ (HELPER) ---
    def clean_text(self, text):
        text = text.replace("passage: ", "") # Lo·∫°i b·ªè prefix c·ªßa E5
        text = re.sub(r'[-_=*]{3,}', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    def get_source_filter(self, query):
        """Logic l·ªçc file th√¥ng minh d·ª±a tr√™n t·ª´ kh√≥a"""
        query_lower = query.lower()
        target_files = set()

        # ƒê·ªãnh nghƒ©a mapping file
        f_blds = "BLDS.docx"
        f_blhs = "BLHS.docx"
        f_bltths = "BLTTHS.docx"
        f_lgtdb = "LGTDB.docx"
        f_anm = "Lu·∫≠t-An-Ninh-M·∫°ng.docx"
        f_nd168 = "ND168.docx"
        f_nd53 = "Ngh·ªã-ƒë·ªãnh-53-ND-CP.docx"

        keyword_map = {
            "an_ninh_mang": ([f_anm, f_nd53], ["an ninh m·∫°ng", "hacker", "virus", "m√£ ƒë·ªôc", "d·ªØ li·ªáu c√° nh√¢n", "x√∫c ph·∫°m", "facebook", "zalo"]),
            "giao_thong": ([f_lgtdb, f_nd168], ["giao th√¥ng", "lgtƒëb", "l√°i xe", "ƒë√®n ƒë·ªè", "n·ªìng ƒë·ªô c·ªìn", "r∆∞·ª£u bia", "t∆∞·ªõc b·∫±ng", "ph·∫°t ngu·ªôi", "m≈© b·∫£o hi·ªÉm"]),
            "hinh_su": ([f_blhs, f_bltths], ["h√¨nh s·ª±", "blhs", "t√π", "gi·∫øt ng∆∞·ªùi", "tr·ªôm c·∫Øp", "c∆∞·ªõp", "l·ª´a ƒë·∫£o", "ma t√∫y", "ƒë√°nh b·∫°c", "kh·ªüi t·ªë", "b·ªã can"]),
            "dan_su": ([f_blds], ["d√¢n s·ª±", "blds", "h·ª£p ƒë·ªìng", "b·ªìi th∆∞·ªùng", "th·ª´a k·∫ø", "di ch√∫c", "ƒë·∫•t ƒëai", "ly h√¥n", "vay n·ª£"])
        }

        for _, (files, keywords) in keyword_map.items():
            if any(k in query_lower for k in keywords):
                target_files.update(files)
        
        if not target_files:
            return None
        
        target_list = list(target_files)
        # C√∫ ph√°p l·ªçc c·ªßa ChromaDB
        if len(target_list) == 1:
            return {"source_name": {"$eq": target_list[0]}}
        return {"source_name": {"$in": target_list}}

    def advanced_retrieval(self, query, metadata_filter, top_k_final=3):
        """Vector Search -> Cross-Encoder Rerank"""
        # B1: L·∫•y r·ªông (top 15)
        initial_docs = self.vectorstore.similarity_search(
            f"query: {query}", 
            k=15, 
            filter=metadata_filter
        )
        if not initial_docs: return []

        # B2: Rerank
        doc_contents = [self.clean_text(d.page_content) for d in initial_docs]
        pairs = [[query, content] for content in doc_contents]
        scores = self.reranker.predict(pairs)
        
        # B3: Sort & Filter
        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)
        final_docs = []
        
        for doc, score in scored_docs[:top_k_final]:
            if score > -5.0: # Ng∆∞·ª°ng ch·∫•p nh·∫≠n
                doc.metadata['score'] = float(score)
                final_docs.append(doc)
                
        return final_docs

    # --- H√ÄM CH√çNH ƒê∆Ø·ª¢C API G·ªåI ---
    def query(self, question: str, k: int = 3):
        try:
            query_str = unicodedata.normalize("NFC", question.strip())
            
            # 1. T√¨m t√†i li·ªáu
            metadata_filter = self.get_source_filter(query_str)
            docs = self.advanced_retrieval(query_str, metadata_filter, top_k_final=k)
            
            if not docs:
                return {'answer': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin lu·∫≠t ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu.', 'sources': []}

            # 2. T·∫°o Context XML
            context_text = "<documents>\n"
            for i, doc in enumerate(docs):
                clean_content = self.clean_text(doc.page_content)
                source = doc.metadata.get('source_name', 'Unknown')
                context_text += f'<doc id="{i+1}" source="{source}">\n{clean_content}\n</doc>\n'
            context_text += "</documents>"

            # 3. G·ªçi LLM tr·∫£ l·ªùi
            formatted_prompt = self.prompt_template.format(context=context_text, question=query_str)
            answer = self.llm.invoke(formatted_prompt)

            # 4. Format ngu·ªìn ƒë·ªÉ tr·∫£ v·ªÅ API
            sources = []
            for d in docs:
                sources.append({
                    'content': self.clean_text(d.page_content)[:200] + '...',
                    'metadata': d.metadata
                })

            return {'answer': answer, 'sources': sources}

        except Exception as e:
            print(f"‚ùå Error RAG: {e}")
            import traceback
            traceback.print_exc()
            return {'answer': 'L·ªói h·ªá th·ªëng khi x·ª≠ l√Ω c√¢u h·ªèi.', 'sources': [], 'error': str(e)}

_rag_service = None

def get_rag_service():
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service