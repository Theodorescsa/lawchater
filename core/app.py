import os
import unicodedata
import torch
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage

os.environ["ANONYMIZED_TELEMETRY"] = "False"

class RAGService:
    _instance = None
    _initialized = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(RAGService, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._initialize()
            self._initialized = True

    def _initialize(self):
        print("üî• ƒêang kh·ªüi t·∫°o RAG Service (Optimized for GTX 1650)...")
        
        # 1. LLM (Gi·∫£m max_tokens ƒë·ªÉ ph·∫£n h·ªìi nhanh h∆°n)
        LLM_BASE_URL = os.getenv("LLM_BASE_URL", "http://llama:8080/v1")
        self.llm = ChatOpenAI(
            model="qwen1_5-1_8b-chat-q8_0",
            base_url=LLM_BASE_URL,
            api_key="not-needed",
            temperature=0.3,
            max_tokens=512, # Gi·∫£m xu·ªëng m·ª©c v·ª´a ƒë·ªß ƒë·ªçc
            model_kwargs={"stop": ["Question:", "C√¢u h·ªèi:", "<|im_end|>"]}
        )

        # 2. Embedding
        EMBEDDING_MODEL = "intfloat/multilingual-e5-small"
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={"device": device}
        )

        # 3. Vectorstore
        # ƒê·∫£m b·∫£o d√πng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·ªÉ tr√°nh l·ªói path
        ABS_DATA_PATH = os.path.abspath("./core/data")
        PERSIST_PATH = "./core/chroma_db"
        
        self.vectorstore = Chroma(
            collection_name="law_docs",
            persist_directory=PERSIST_PATH,
            embedding_function=self.embedding_model
        )
        
        # Mapping t·ª´ kh√≥a sang t√™n file (L∆∞u √Ω: B·∫°n c·∫ßn ƒë·∫£m b·∫£o t√™n file ch√≠nh x√°c)
        self.topic_mapping = {
            "h√¥n nh√¢n": ["Lu·∫≠t-H√¥n-nh√¢n-v√†-gia-ƒë√¨nh.docx", "Ngh·ªã-quy·∫øt-326.docx"],
            "ly h√¥n": ["Lu·∫≠t-H√¥n-nh√¢n-v√†-gia-ƒë√¨nh.docx", "Ngh·ªã-quy·∫øt-326.docx", "BLDS.docx"],
            "ƒë·∫•t ƒëai": ["Lu·∫≠t-ƒê·∫•t-ƒëai.docx"],
            "h√¨nh s·ª±": ["BLHS.docx", "BLTTHS.docx"],
            "t√π": ["BLHS.docx", "BLTTHS.docx"],
            "d√¢n s·ª±": ["BLDS.docx"],
            "giao th√¥ng": ["LGTDB.docx", "ND168.docx"],
            "ph·∫°t ngu·ªôi": ["ND168.docx"],
        }
        
        # Prompt ng·∫Øn g·ªçn h∆°n ƒë·ªÉ x·ª≠ l√Ω nhanh h∆°n
        self.prompt_template = ChatPromptTemplate.from_template("""D·ª±a v√†o lu·∫≠t sau:
---
{context}
---
Tr·∫£ l·ªùi c√¢u h·ªèi: {input}
(Ch·ªâ tr·∫£ l·ªùi d·ª±a v√†o n·ªôi dung tr√™n. Ng·∫Øn g·ªçn, s√∫c t√≠ch).""")

    def get_smart_filter(self, query):
        """Tr·∫£ v·ªÅ list file ti·ªÅm nƒÉng d·ª±a tr√™n t·ª´ kh√≥a"""
        query_lower = query.lower()
        target_files = set()
        
        # L·∫•y ƒë∆∞·ªùng d·∫´n g·ªëc ƒë·ªÉ t·∫°o filter path ch√≠nh x√°c
        abs_data_path = os.path.abspath("./core/data")

        for keyword, filenames in self.topic_mapping.items():
            if keyword in query_lower:
                for fname in filenames:
                    # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß kh·ªõp v·ªõi l√∫c Ingest
                    full_path = os.path.join(abs_data_path, fname)
                    target_files.add(full_path)
        
        if not target_files:
            return None
            
        if len(target_files) == 1:
            return {"source": {"$eq": list(target_files)[0]}}
        return {"source": {"$in": list(target_files)}}

    def query(self, question: str, k: int = 3):
        try:
            query = unicodedata.normalize("NFC", question.strip())
            print(f"üìù Query: {query}")

            # --- CHI·∫æN THU·∫¨T SMART FILTER ---
            docs = []
            
            # B∆∞·ªõc 1: Th·ª≠ t√¨m v·ªõi Filter (Nhanh nh·∫•t)
            metadata_filter = self.get_smart_filter(query)
            if metadata_filter:
                print("üéØ ƒêang t√¨m ki·∫øm v·ªõi Smart Filter...")
                retriever = self.vectorstore.as_retriever(
                    search_kwargs={"k": k, "filter": metadata_filter}
                )
                docs = retriever.invoke(query)
            
            # B∆∞·ªõc 2: Fallback - N·∫øu kh√¥ng th·∫•y docs n√†o, t√¨m to√†n b·ªô (An to√†n)
            if not docs:
                print("üåê Filter kh√¥ng ra k·∫øt qu·∫£ -> T√¨m ki·∫øm to√†n b·ªô DB...")
                retriever_full = self.vectorstore.as_retriever(search_kwargs={"k": k})
                docs = retriever_full.invoke(query)

            print(f"‚úÖ T√¨m th·∫•y {len(docs)} documents")
            
            if not docs:
                return {'answer': 'Kh√¥ng t√¨m th·∫•y th√¥ng tin lu·∫≠t ph√π h·ª£p.', 'sources': []}

            # T·∫°o context
            context = "\n\n".join([d.page_content for d in docs])
            
            # G·ªçi LLM
            formatted = self.prompt_template.format(context=context, input=query)
            messages = [
                SystemMessage(content="B·∫°n l√† tr·ª£ l√Ω ph√°p lu·∫≠t."),
                HumanMessage(content=formatted)
            ]
            
            resp = self.llm.invoke(messages)
            answer = resp.content if hasattr(resp, "content") else str(resp)

            # Sources
            sources = [{'content': d.page_content[:150] + '...', 'metadata': d.metadata} for d in docs]

            return {'answer': answer, 'sources': sources}

        except Exception as e:
            print(f"‚ùå Error: {e}")
            return {'answer': 'L·ªói h·ªá th·ªëng.', 'sources': [], 'error': str(e)}

_rag_service = None

def get_rag_service():
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service