import os
import shutil
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma

os.environ["ANONYMIZED_TELEMETRY"] = "False"

# X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n
BASE_DIR = Path(__file__).resolve().parent
DATA_PATH = str(BASE_DIR / "data")
PERSIST_PATH = str(BASE_DIR / "chroma_db")
COLLECTION_NAME = "law_docs"
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-small"

def main():
    print("üöÄ B·∫Øt ƒë·∫ßu n·∫°p d·ªØ li·ªáu (Ingest)...")
    
    if not os.path.exists(DATA_PATH):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y folder data t·∫°i {DATA_PATH}")
        return

    # 1. Load Documents
    documents = []
    def load_docs(glob, loader_cls):
        loader = DirectoryLoader(DATA_PATH, glob=glob, loader_cls=loader_cls, show_progress=True)
        try: return loader.load()
        except: return []

    documents.extend(load_docs("**/*.pdf", PyPDFLoader))
    documents.extend(load_docs("**/*.docx", Docx2txtLoader))
    
    if not documents:
        print("‚ùå Folder data r·ªóng.")
        return

    # 2. Ti·ªÅn x·ª≠ l√Ω (Th√™m t√™n file v√†o n·ªôi dung)
    print("üõ†Ô∏è ƒêang x·ª≠ l√Ω metadata...")
    for doc in documents:
        source_file = os.path.basename(doc.metadata.get('source', ''))
        doc.metadata['source_name'] = source_file
        # G·∫Øn t√™n file v√†o n·ªôi dung ƒë·ªÉ chunk n√†o c≈©ng bi·∫øt m√¨nh thu·ªôc lu·∫≠t n√†o
        doc.page_content = f"T√†i li·ªáu: {source_file}\n{doc.page_content}"

    # 3. Chia nh·ªè (Split)
    print("‚úÇÔ∏è ƒêang chia nh·ªè vƒÉn b·∫£n...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000, 
        chunk_overlap=200,
        separators=["\n\nƒêi·ªÅu ", "\nƒêi·ªÅu ", "ƒêi·ªÅu "],
        keep_separator=True
    )
    chunks = splitter.split_documents(documents)

    # 4. Th√™m prefix cho E5 Model
    final_chunks = []
    for chunk in chunks:
        # B·∫Øt bu·ªôc cho model intfloat/multilingual-e5-small
        chunk.page_content = f"passage: {chunk.page_content}"
        final_chunks.append(chunk)

    # 5. Embedding & L∆∞u v√†o ChromaDB
    print("üíæ ƒêang ghi v√†o Database...")
    embedding = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cpu'})
    
#    X√≥a DB c≈© n·∫øu c√≥ ƒë·ªÉ l√†m s·∫°ch (FIX L·ªñI DEVICE BUSY)
    if os.path.exists(PERSIST_PATH):
        # Thay v√¨ x√≥a th∆∞ m·ª•c (g√¢y l·ªói n·∫øu l√† mount point), ta x√≥a n·ªôi dung b√™n trong
        for filename in os.listdir(PERSIST_PATH):
            file_path = os.path.join(PERSIST_PATH, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)  # X√≥a file
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)  # X√≥a th∆∞ m·ª•c con
            except Exception as e:
                print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng th·ªÉ x√≥a {file_path}. L·ªói: {e}")

    Chroma.from_documents(
        documents=final_chunks, 
        embedding=embedding, 
        collection_name=COLLECTION_NAME, 
        persist_directory=PERSIST_PATH
    )
    
    print(f"‚úÖ Ho√†n t·∫•t! ƒê√£ l∆∞u {len(final_chunks)} chunks v√†o {PERSIST_PATH}")

if __name__ == "__main__":
    main()