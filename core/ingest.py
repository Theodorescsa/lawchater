import os
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_chroma import Chroma

os.environ["ANONYMIZED_TELEMETRY"] = "False"

# L·∫•y ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi c·ªßa th∆∞ m·ª•c core
BASE_DIR = Path(__file__).resolve().parent

DATA_PATH = str(BASE_DIR / "data")
PERSIST_PATH = str(BASE_DIR / "chroma_db")
COLLECTION_NAME = "law_docs"
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-small"

def main():
    print("B·∫Øt ƒë·∫ßu qu√° tr√¨nh n·∫°p d·ªØ li·ªáu...")
    print(f"ƒê∆∞·ªùng d·∫´n data: {DATA_PATH}")
    print(f"ƒê∆∞·ªùng d·∫´n persist: {PERSIST_PATH}")
    
    # Ki·ªÉm tra th∆∞ m·ª•c data c√≥ t·ªìn t·∫°i kh√¥ng
    if not os.path.exists(DATA_PATH):
        print(f"‚ùå L·ªñI: Th∆∞ m·ª•c '{DATA_PATH}' kh√¥ng t·ªìn t·∫°i!")
        return
    
    # Li·ªát k√™ c√°c file trong th∆∞ m·ª•c data
    print(f"\nC√°c file trong th∆∞ m·ª•c data:")
    for file in os.listdir(DATA_PATH):
        print(f"  - {file}")
    print()

    # --- T·∫£i t√†i li·ªáu ---
    documents = []
    
    def load_docs(glob_pattern, loader_cls, label):
        print(f"ƒêang t·∫£i file {label} t·ª´ '{DATA_PATH}'...")
        loader = DirectoryLoader(
            DATA_PATH,
            glob=glob_pattern,
            loader_cls=loader_cls,
            use_multithreading=True,
            show_progress=True
        )
        try:
            docs = loader.load()
            print(f"‚úÖ ƒê√£ t·∫£i {len(docs)} t√†i li·ªáu {label}")
            return docs
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi t·∫£i {label}: {e}")
            return []

    documents.extend(load_docs("**/*.pdf", PyPDFLoader, "PDF"))
    documents.extend(load_docs("**/*.docx", Docx2txtLoader, "DOCX"))
    documents.extend(load_docs("**/*.doc", Docx2txtLoader, "DOC"))

    if not documents:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o trong th∆∞ m·ª•c '{DATA_PATH}'.")
        print("Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n v√† c√°c file.")
        return

    print(f"\n‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng {len(documents)} t√†i li·ªáu.")
    print(f"T·ªïng s·ªë k√Ω t·ª±: {sum(len(doc.page_content) for doc in documents)}")

    print("\nƒêang chia t√†i li·ªáu th√†nh c√°c m·∫£nh theo 'ƒêi·ªÅu'...")
    
    logical_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=200,
        separators=["\n\nƒêi·ªÅu ", "\nƒêi·ªÅu ", "ƒêi·ªÅu "],
        keep_separator=True
    )
    
    chunks_with_preamble = logical_splitter.split_documents(documents)
    
    final_chunks = []
    for chunk in chunks_with_preamble:
        content = chunk.page_content.lstrip()
        if content.startswith("ƒêi·ªÅu "):
            chunk.page_content = content
            final_chunks.append(chunk)

    if not final_chunks:
        print("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y 'ƒêi·ªÅu ' n√†o trong vƒÉn b·∫£n.")
        print("S·ª≠ d·ª•ng t·∫•t c·∫£ c√°c chunks...")
        final_chunks = chunks_with_preamble

    print(f"‚úÖ ƒê√£ chia th√†nh {len(final_chunks)} m·∫£nh logic.")

    print(f"\nƒêang t·∫£i m√¥ h√¨nh embedding '{EMBEDDING_MODEL_NAME}'...")
    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'}
    )
    print("‚úÖ ƒê√£ t·∫£i embedding model")

    print(f"\nƒêang ghi d·ªØ li·ªáu v√†o ChromaDB (collection: {COLLECTION_NAME})...")
    print("‚è≥ Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...")

    try:
        vectorstore = Chroma.from_documents(
            documents=final_chunks,            
            embedding=embedding_model,          
            collection_name=COLLECTION_NAME,    
            persist_directory=PERSIST_PATH    
        )
        print("‚úÖ ƒê√£ ghi d·ªØ li·ªáu v√†o ChromaDB")
    except Exception as e:
        print(f"‚ùå L·ªói khi ghi v√†o ChromaDB: {e}")
        import traceback
        traceback.print_exc()
        return

    print("\n" + "=" * 60)
    print(f"üéâ Ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{PERSIST_PATH}'")
    print(f"üìä T·ªïng s·ªë chunks: {len(final_chunks)}")
    print(f"üìö Collection name: {COLLECTION_NAME}")
    print("=" * 60)

if __name__ == "__main__":
    main()